{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librairies que nous allons utiliser\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_boston\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "import cma\n",
    "import cma.purecma as purecma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ici, nous avons crée des données jouets qui servirons d'entrainement pour apprendre les différentes façons\n",
    "d'entrainer un modèle a réduire une fitness.\n",
    "Donc notre fitnesse c'est \n",
    "\"\"\"\n",
    "#Creations de données jouées \n",
    "x = np.linspace(-20,20,30).reshape((-1,1))\n",
    "y = 5 * x + 3 + np.random.normal(0,5,x.shape)\n",
    "y = y.reshape((-1,1))\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tracage d'une droite dans un le nuage de point\n",
    "\"\"\"\n",
    "Supposons que l'on ait trouvé un modèle w = a,b tel que la prédiction serrait égale y = ax + b.\n",
    "Pour le tester on pourra utiliser cette fonction.\n",
    "Remarque : w doit être un vecteur colonne et x un vecteur ligne pour que le produit sur les différents x\n",
    "donne les prédictions du modèle.\n",
    "\"\"\"\n",
    "\n",
    "def predictions(w,x):\n",
    "    ypredict = np.dot(x,w.T)\n",
    "    return ypredict\n",
    "    \n",
    "\"\"\"\n",
    "Cette fonction ajoute simple une colonne de 1 pour que chaque produit de x par un w donne x0 * a + b *1 c'est a\n",
    "a dire a*x0+b.\n",
    "\"\"\"\n",
    "def adapt(X):\n",
    "    return np.hstack((X,np.ones((X.shape[0],1))))  \n",
    "\n",
    "#D'abord on affiche le nuage\n",
    "plt.scatter(x,y)\n",
    "#Ensuite on ajoute le tracé\n",
    "ypred = predictions(np.array([5,2]),adapt(x)) #Donc la je test a = 5, b = 2 parce que je les connaits\n",
    "plt.plot(x,ypred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Résolution direct d'une regression linéaire\n",
    "\"\"\"\n",
    "Donc la l'objectif clairement c'est de retourver les coefficients de la droite de regression linéaire en utilisant\n",
    "une résolution algébrique.\n",
    "Rappel : \n",
    "    - Résoudre un systeme Xw = B reviens a trouver x c'est a dire A^-1*B (donc calculer l'inverse de A par B)\n",
    "    - En Python, pour calculer l'inverse on fait : np.linalg.pinv(A) (pseudo-inversion)\n",
    "\"\"\"\n",
    "w = np.dot(np.linalg.pinv(adapt(x).T.dot(adapt(x))),adapt(x).T.dot(y))\n",
    "print(\"Donc le modèle qu'on trouve est : \",w.T)\n",
    "plt.scatter(x,y)\n",
    "#Calcul des prédictions\n",
    "ypred = predictions(w.T,adapt(x))\n",
    "#Tracage du modèle : \n",
    "plt.plot(x,ypred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résolution par descente de gradient\n",
    "Le gradient d'une fonction de plusieurs paramètres correspond au vecteur des dérivées suivant les différents paramètres, il indique le sens dans lequel la fonction croit.\n",
    "Par exemple : \n",
    "Soit f(x,y,z) = 3x + 8y + 3z² + 6 le gradient est \n",
    "$ \\nabla $f(x,y,z) = (3,8,6z) \n",
    "et donc le gradient donne pour un point donné $\\nabla$f(4,5,2) = (3,8,12).\n",
    "Donc pour un coût égale a la différence entre la prédiction et la valeur réelle au carré le calcul se fait en version matricielle comme suit :\n",
    "\n",
    "Cout des prédictions : C'est la différence entre la prédiction et la valeur qui donne un vecteur Xw - y\n",
    "\n",
    "Donc le cout est $(Xw-y)^T * (Xw-y)$ (multiplier une ligne par une colonne sur un papier pour s'en convaincre)\n",
    "\n",
    "Et sa dérivée est la dérivée selon w qui donne : 2X(Xw-y) et il suffit de remplacer n'importe quel w pour avoir le vecteur vers lequel ce coût augmente (et par inversion celui vers lequel le coût diminue).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X,y,w):\n",
    "    return np.dot(X,w)- y\n",
    "\n",
    "def gradient(X,y,w):\n",
    "    return np.dot(2*X.T,np.dot(X,w)- y)\n",
    "\n",
    "\n",
    "x = np.linspace(-20,20,30).reshape((-1,1))\n",
    "y = 5 * x + 3 + np.random.normal(0,5,x.shape)\n",
    "y = y.reshape((-1,1))\n",
    "\n",
    "#J'initialise les poids aléatoirement\n",
    "w = np.zeros((1,2)) #1 Ligne, Deux colonnes donc un vecteur [a,b]\n",
    "print(w)\n",
    "#On fixe un nombre d'itérations\n",
    "nb_iter = 10\n",
    "x_a = adapt(x)\n",
    "alpha = 1e-5\n",
    "w = w.T\n",
    "for i in range(10000):\n",
    "    #Je met a jour les poids en suivant l'inverse du gradient par alpha\n",
    "    pass\n",
    "print(\"w = \",w)\n",
    "w = np.dot(np.linalg.pinv(x_a.T.dot(x_a)),x_a.T.dot(y))\n",
    "print(\"Et le modèle qu'on trouve par résolution directe est : \",w.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modéle plus compliqué\n",
    "Une fois qu'on a compris une façon d'apprendre a paramétrer un modèle pour réduire un objectif (donné) il faut garder a l'esprit que le modèle linéaire est simplement un exemple de modèles.\n",
    "On peut facilement l'etendre de plusieurs façons : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Par exemple si on ajoute une colonne a X correspondant a x² en plus de la colonne de 1 et qu'on calcule \n",
    "un w = [a,b,c], Xw donne ax² + bx + 1 donc c'est un genre de régression quadratique.\n",
    "Et ça permet d'apprendre une forme de parabole\n",
    "\"\"\"\n",
    "x = np.linspace(-20,20,30).reshape((-1,1))\n",
    "y = 5 * x**2 + 6*x + 3 + np.random.normal(0,30,x.shape)\n",
    "y = y.reshape((-1,1))\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adapt_quad(X):\n",
    "    return np.hstack((X**2,X,np.ones((X.shape[0],1))))  \n",
    "\n",
    "#Determiner directement le wecteur de poids en utilisant l'adaptation quadratique\n",
    "\n",
    "\n",
    "print(\"Donc le modèle qu'on trouve est : \",w.T)\n",
    "plt.scatter(x,y)\n",
    "#Calcul des prédictions\n",
    "ypred = predictions(w.T,adapt_quad(x))\n",
    "#Tracage du modèle : \n",
    "plt.plot(x,ypred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,+5,30).reshape((-1,1))\n",
    "y = np.sin(x) \n",
    "y = y.reshape((-1,1))\n",
    "\n",
    "def adapt_quad(X):\n",
    "    return np.hstack((X**2,X,np.ones((X.shape[0],1))))  \n",
    "\n",
    "w = np.dot(np.linalg.pinv(adapt_quad(x).T.dot(adapt_quad(x))),adapt_quad(x).T.dot(y))\n",
    "print(\"Donc le modèle qu'on trouve est : \",w.T)\n",
    "plt.scatter(x,y)\n",
    "#Calcul des prédictions\n",
    "ypred = predictions(w.T,adapt_quad(x))\n",
    "#Tracage du modèle : \n",
    "plt.plot(x,ypred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,+5,30).reshape((-1,1))\n",
    "y = np.sin(x) \n",
    "y = y.reshape((-1,1))\n",
    "\n",
    "def adapt_quad(X):\n",
    "    return np.hstack((X**3,X**2,X,np.ones((X.shape[0],1))))  \n",
    "\n",
    "w = np.dot(np.linalg.pinv(adapt_quad(x).T.dot(adapt_quad(x))),adapt_quad(x).T.dot(y))\n",
    "print(\"Donc le modèle qu'on trouve est : \",w.T)\n",
    "plt.scatter(x,y)\n",
    "#Calcul des prédictions\n",
    "ypred = predictions(w.T,adapt_quad(x))\n",
    "#Tracage du modèle : \n",
    "plt.plot(x,ypred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,+5,30).reshape((-1,1))\n",
    "y = np.sin(x) \n",
    "y = y.reshape((-1,1))\n",
    "\n",
    "def adapt_quad(X):\n",
    "    #Proposez une amélioration de l'adaptation qui permette de coller le mieux au données sans trop augmenter\n",
    "    #leur taille.\n",
    "    return 0\n",
    "\n",
    "w = np.dot(np.linalg.pinv(adapt_quad(x).T.dot(adapt_quad(x))),adapt_quad(x).T.dot(y))\n",
    "print(\"Donc le modèle qu'on trouve est : \",w.T)\n",
    "plt.scatter(x,y)\n",
    "#Calcul des prédictions\n",
    "ypred = predictions(w.T,adapt_quad(x))\n",
    "#Tracage du modèle : \n",
    "plt.plot(x,ypred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,+10,30).reshape((-1,1))\n",
    "y = np.sin(x) \n",
    "y = y.reshape((-1,1))\n",
    "\n",
    "def adapt_quad(X):\n",
    "    #Reprenez l'adaptation utilisée précédemment sur la fonction sinus sur l'intervalle -10,10\n",
    "    #Que constatez vous ?\n",
    "\n",
    "w = np.dot(np.linalg.pinv(adapt_quad(x).T.dot(adapt_quad(x))),adapt_quad(x).T.dot(y))\n",
    "print(\"Donc le modèle qu'on trouve est : \",w.T)\n",
    "plt.scatter(x,y)\n",
    "#Calcul des prédictions\n",
    "ypred = predictions(w.T,adapt_quad(x))\n",
    "#Tracage du modèle : \n",
    "plt.plot(x,ypred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Démystifier les Réseaux de Neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un Perceptron \n",
    "Enfaite un perceptron c'est juste une regression linéaire qui en sortie passe par une fonction d'activation,\n",
    "En l'occurrent si la fonction d'activation est la fonction identité, c'est exactement une regression linéaire.\n",
    "Ce qu'il faut comprendre c'est que chaque neurone dispose d'une matrice de pondérations pour les synapses en entrée \n",
    "![40% center](nn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de commencer a expérimenter, on importe un dataset, on en profite également pour importer une fonction de\n",
    "coûts implémentée par Scikit-Learn qui calcule les moindres carrés , ~~la flemme de la coder~~ les détails de son\n",
    "implémentation sont laissés en exercice au lecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Donc pour rappel un réseau de neurones se compose de : \n",
    "    - Une matrice de poids pour les synapses en entrée \n",
    "    - Une matrice de biais\n",
    "    - Une fonction d'activation qui détermine la sortie en fonction d'une combinaison linéaire de l'entrée.\n",
    "Calculer la sortie d'un neurone (ou d'un réseau de neurones) se fait en utilisant l'algorithme de feed forward. \n",
    "\"\"\"\n",
    "\n",
    "weights = np.zeros((13,1)) #On met les weights a zéro par exemple.\n",
    "biais = np.zeros(1)\n",
    "\n",
    "sigm = lambda x:1/(1 + np.exp(-x))\n",
    "identity = lambda x:x\n",
    "\n",
    "def feed_forward(x,*neurone):\n",
    "    w,biais,activation_func = neurone\n",
    "    y = np.dot(x,w) + biais\n",
    "    y = activation_func(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imaginons que la par exemple on entraine nos pondérations par résolution directe des moindres carrés\n",
    "\"\"\"\n",
    "weights = np.dot(np.linalg.pinv(adapt(X).T.dot(adapt(X))),adapt(X).T.dot(y))\n",
    "print(\"Weights = \",weights)\n",
    "\"\"\"\n",
    "A ce moment la la prédiction vas normalement être plus précise\n",
    "Remarque : si vous comptez vous remarquerez qu'il y'a 14 weights et qu'il devrait y'en avoir 13 je vous laisse\n",
    "refléchir a pourquoi. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Réponse : \n",
    "\"\"\"\n",
    "weights , biais = weights[:-1],weights[-1]\n",
    "\"\"\"\n",
    "Affichage du résultat\n",
    "\"\"\"\n",
    "print(\"prediction : \",feed_forward(X[0],weights,biais,identity), \" la vraie valeur est : \", y[0])\n",
    "print(\"Erreurs globales : \", mean_squared_error(feed_forward(X,weights,biais,identity),y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras : Un exemple de librairies pour les réseaux de neurones \n",
    "Ici je vous présente Keras une librairie utilisée pour manipuler les réseaux de neurones multi-couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cette classe contient une représentation plus complète d'un réseau de neurones multicouches.\n",
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "x = np.linspace(-200,200,1000).reshape((-1,1))\n",
    "y = 5 * x + 3 + np.random.normal(0,5,x.shape)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1, input_shape=(1,)),\n",
    "    Activation('linear'),\n",
    "    Dense(10,),\n",
    "    Activation('linear'),\n",
    "    Dense(1,),\n",
    "    Activation('linear'),\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(x, y, epochs=100, batch_size=32)\n",
    "#Calcul des prédictions\n",
    "y_pred = model.predict(x)\n",
    "#Tracage du modèle : \n",
    "plt.plot(x,y_pred)\n",
    "\n",
    "x = np.linspace(-200,200,50).reshape((-1,1))\n",
    "y = 5 * x + 3 + np.random.normal(0,5,x.shape)\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va tester ça sur l'exemple de tout a l'heure a présent : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "model = Sequential([\n",
    "    Dense(1, input_shape=(13,)),\n",
    "    Activation('linear'),\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "model.fit(X, y, epochs=10000, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que ça prend quand même pas mal de temps, et que c'est moins précis qu'une résolution directe alors pourquoi s'en servir plûtot que la résolution directe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "model = Sequential([\n",
    "    Dense(1, input_shape=(13,)),\n",
    "    Activation('linear'),\n",
    "    Dense(64,),\n",
    "    Activation('relu'),\n",
    "    Dense(64,),\n",
    "    Activation('relu'),\n",
    "    Dense(1,),\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "model.fit(X, y, epochs=5000, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "x = np.linspace(-10,10,1000).reshape((-1,1))\n",
    "y = np.sinh(x) + np.random.normal(0,100,x.shape)\n",
    "\n",
    "w = np.dot(np.linalg.pinv(adapt(x).T.dot(adapt(x))),adapt(x).T.dot(y))\n",
    "print(\"Donc le modèle qu'on trouve est : \",w.T)\n",
    "plt.scatter(x,y)\n",
    "#Calcul des prédictions\n",
    "ypred = predictions(w.T,adapt(x))\n",
    "\n",
    "#Tracage du modèle : \n",
    "plt.scatter(x,y,color=\"blue\")\n",
    "plt.plot(x,ypred,color = \"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "x = np.linspace(-5,5,1000).reshape((-1,1))\n",
    "y = np.sinh(x) + np.random.normal(0,5,x.shape)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1, input_shape=(1,)),\n",
    "    Dense(64,),\n",
    "    Activation('relu'),\n",
    "    Dense(64,),\n",
    "    Activation('relu'),\n",
    "    Dense(1,),\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(x, y, epochs=100, batch_size=100)\n",
    "#Calcul des prédictions\n",
    "y_pred = model.predict(x)\n",
    "#Tracage du modèle : \n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,y_pred,color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipuler un multi-couche en flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "class SimpleNeuralControllerNumpy():\n",
    "    def __init__(self, n_in, n_out, n_hidden_layers=2, n_neurons_per_hidden=5, params=None):\n",
    "        self.dim_in = n_in\n",
    "        self.dim_out = n_out\n",
    "        # if params is provided, we look for the number of hidden layers and neuron per layer into that parameter (a dicttionary)\n",
    "        if (not params==None):\n",
    "            if (\"n_hidden_layers\" in params.keys()):\n",
    "                n_hidden_layers=params[\"n_hidden_layers\"]\n",
    "            if (\"n_neurons_per_hidden\" in params.keys()):\n",
    "                n_neurons_per_hidden=params[\"n_neurons_per_hidden\"]\n",
    "        self.n_per_hidden = n_neurons_per_hidden\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.weights = None \n",
    "        self.n_weights = None\n",
    "        self.init_random_params()\n",
    "        self.out = np.zeros(n_out)\n",
    "        #print(\"Creating a simple mlp with %d inputs, %d outputs, %d hidden layers and %d neurons per layer\"%(n_in, n_out,n_hidden_layers, n_neurons_per_hidden))\n",
    "    def init_random_params(self):\n",
    "        if(self.n_hidden_layers > 0):\n",
    "            self.weights = [np.random.random((self.dim_in,self.n_per_hidden))] # In -> first hidden\n",
    "            self.bias = [np.random.random(self.n_per_hidden)] # In -> first hidden\n",
    "            for i in range(self.n_hidden_layers-1): # Hidden -> hidden\n",
    "                self.weights.append(np.random.random((self.n_per_hidden,self.n_per_hidden)))\n",
    "                self.bias.append(np.random.random(self.n_per_hidden))\n",
    "            self.weights.append(np.random.random((self.n_per_hidden,self.dim_out))) # -> last hidden -> out\n",
    "            self.bias.append(np.random.random(self.dim_out))\n",
    "        else:\n",
    "            self.weights = [np.random.random((self.dim_in,self.dim_out))] # Single-layer perceptron\n",
    "            self.bias = [np.random.random(self.dim_out)]\n",
    "        self.n_weights = np.sum([np.product(w.shape) for w in self.weights]) + np.sum([np.product(b.shape) for b in self.bias])\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns all network parameters as a single array\n",
    "        \"\"\"\n",
    "        flat_weights = np.hstack([arr.flatten() for arr in (self.weights+self.bias)])\n",
    "        return flat_weights\n",
    "\n",
    "    def set_parameters(self, flat_parameters):\n",
    "        \"\"\"\n",
    "        Set all network parameters from a single array\n",
    "        \"\"\"\n",
    "        i = 0 # index\n",
    "        to_set = []\n",
    "        self.weights = list()\n",
    "        self.bias = list()\n",
    "        if(self.n_hidden_layers > 0):\n",
    "            # In -> first hidden\n",
    "            w0 = np.array(flat_parameters[i:(i+self.dim_in*self.n_per_hidden)])\n",
    "            self.weights.append(w0.reshape(self.dim_in,self.n_per_hidden))\n",
    "            i += self.dim_in*self.n_per_hidden\n",
    "            for l in range(self.n_hidden_layers-1): # Hidden -> hidden\n",
    "                w = np.array(flat_parameters[i:(i+self.n_per_hidden*self.n_per_hidden)])\n",
    "                self.weights.append(w.reshape((self.n_per_hidden,self.n_per_hidden)))\n",
    "                i += self.n_per_hidden*self.n_per_hidden\n",
    "            # -> last hidden -> out\n",
    "            wN = np.array(flat_parameters[i:(i+self.n_per_hidden*self.dim_out)])\n",
    "            self.weights.append(wN.reshape((self.n_per_hidden,self.dim_out)))\n",
    "            i += self.n_per_hidden*self.dim_out\n",
    "            # Samefor bias now\n",
    "            # In -> first hidden\n",
    "            b0 = np.array(flat_parameters[i:(i+self.n_per_hidden)])\n",
    "            self.bias.append(b0)\n",
    "            i += self.n_per_hidden\n",
    "            for l in range(self.n_hidden_layers-1): # Hidden -> hidden\n",
    "                b = np.array(flat_parameters[i:(i+self.n_per_hidden)])\n",
    "                self.bias.append(b)\n",
    "                i += self.n_per_hidden\n",
    "            # -> last hidden -> out\n",
    "            bN = np.array(flat_parameters[i:(i+self.dim_out)])\n",
    "            self.bias.append(bN)\n",
    "            i += self.dim_out\n",
    "        else:\n",
    "            n_w = self.dim_in*self.dim_out\n",
    "            w = np.array(flat_parameters[:n_w])\n",
    "            self.weights = [w.reshape((self.dim_in,self.dim_out))]\n",
    "            self.bias = [np.array(flat_parameters[n_w:])]\n",
    "        self.n_weights = np.sum([np.product(w.shape) for w in self.weights]) + np.sum([np.product(b.shape) for b in self.bias])\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Propagage\n",
    "        \"\"\"\n",
    "        if(self.n_hidden_layers > 0):\n",
    "            #Input\n",
    "            a = np.matmul(x,self.weights[0]) + self.bias[0]\n",
    "            y = sigmoid(a)\n",
    "            # hidden -> hidden\n",
    "            for i in range(1,self.n_hidden_layers-1):\n",
    "                a = np.matmul(y, self.weights[i]) + self.bias[i]\n",
    "                y = sigmoid(a)\n",
    "            # Out\n",
    "            a = np.matmul(y, self.weights[-1]) + self.bias[-1]\n",
    "            out = tanh(a)\n",
    "            return out\n",
    "        else: # Simple monolayer perceptron\n",
    "            return tanh(np.matmul(x,self.weights[0]) + self.bias[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = SimpleNeuralControllerNumpy(10, 1, n_hidden_layers=2, n_neurons_per_hidden=5, params=None)\n",
    "Network.init_random_params()\n",
    "print(Network.get_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
