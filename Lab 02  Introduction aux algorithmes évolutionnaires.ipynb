{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cma\n",
    "import cma.purecma as purecma\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formes Canoniques d'Algorithmes Evolutionnaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nomenclature générale d'un Algorithme Evolutionnaire : \n",
    "Un algorithme evolutionnaire reprend géneralement les composantes suivantes : \n",
    "* Une population d'individus (un ou plusieurs) representant des génotypes \n",
    "* Une méthode de perturbation des individus pour générer des nouveaux individus (successeurs ou offspring)\n",
    "* Une fonction d'évaluation qui sert a comparer les genotypes en fonction de leur phenotypes et a en sélectionner certains.\n",
    "\n",
    "Donc pour chaque élément les variantes les plus communes sont : \n",
    "- Pour la population : 1 Si elle se compose d'un seul élement $\\mu$ si plusieurs et $\\mu / \\sigma$ si parmi la population un nombre $\\sigma$ seulement participent a la créations des successeurs.\n",
    "- Successeurs : 1 ou $\\lambda$ si plusieurs successeurs.\n",
    "- Mode de sélection \",\" si la sélection se fait parmi l'offspring ou \"+\" si elle se fait parmi les parents et l'offspring.\n",
    "\n",
    "Donc par exemple : \n",
    "- (1,1) ne peut pas exister ça voudrait dire qu'on sélectionne parmi une offspring de 1 donc enfaite le nouvel élément serrait seul et on n'aurait pas avec qui le comparer on le prendrait systématiquement\n",
    "- (1+1) signifie qu'on a genere un individu, qu'on perturbe a chaque étape et qu'on garde l'offspring si elle est meilleur que l'individu (donc on effectue une sélection sur le parent et l'offspring).\n",
    "- (1,$\\lambda$) veut dire que a chaque étape on génère plusieures descendants et on remplace le parent systématiquement par le meilleur élément de l'offspring (le parent ne participe pas a la sélection).\n",
    "- ($\\mu$ + $\\lambda$) veut dire que la sélection de la prochaine population qui contient plusieurs individus se fait parmi plusieurs l'offspring et l'ancienne population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmes evolutionnaires basiques\n",
    "Dans cette partie nous allons implémenter et comparer différents algorithmes génétiques simples en les testant sur le problème du Lab 01 sur la prédiction des prix des maisons dans boston."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "On commence par l'algorithme le plus simple qu'on puisse imaginer, un (1+1) avec perturbation normale.\n",
    "\"\"\"\n",
    "def launch_oneplusone(center, sigma, ma_func, nbeval=10000):\n",
    "    parent = np.array(center)\n",
    "    i=0\n",
    "    j=0\n",
    "    parentFit = ma_func(parent)\n",
    "    bestFit = parentFit\n",
    "    bestIt = 0\n",
    "    while i<nbeval:\n",
    "        child = parent + sigma*np.random.normal(0,np.ones(parent.shape),parent.shape)\n",
    "        childFit = ma_func(child)\n",
    "        if childFit <= parentFit:\n",
    "            parentFit = childFit\n",
    "            parent = np.copy(child)\n",
    "            if bestFit > parentFit:\n",
    "                bestFit = parentFit\n",
    "                bestIt = i\n",
    "                print(\"New best fit found : \", bestFit,\" a l'itteration : \",i)\n",
    "\n",
    "        j+=1\n",
    "        solutions = np.array([parent])\n",
    "        i+=1\n",
    "    print (\"Best fit\",bestFit,\"at iteration\",bestIt)\n",
    "    return bestFit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dans ce qui suit nous utiliserons la structure de réseaux de neurones simple que nous avons vu précédement\n",
    "car elle permet de manipuler les poids de façon \"flat\" c'est a dire en amount (de façon non structurée)\n",
    "(Pas la peine de relire tout ça c'est pas important, c'est a titre indicatif et on aurait pu utiliser scikit-learn\n",
    "remarquez juste que j'ai mis une fonction d'activation linéaire a la place de la sigmoid)\n",
    "\"\"\"\n",
    "def sigmoid(x):\n",
    "    return 1./(1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "class SimpleNeuralControllerNumpy():\n",
    "    def __init__(self, n_in, n_out, n_hidden_layers=2, n_neurons_per_hidden=5, params=None):\n",
    "        self.dim_in = n_in\n",
    "        self.dim_out = n_out\n",
    "        # if params is provided, we look for the number of hidden layers and neuron per layer into that parameter (a dicttionary)\n",
    "        if (not params==None):\n",
    "            if (\"n_hidden_layers\" in params.keys()):\n",
    "                n_hidden_layers=params[\"n_hidden_layers\"]\n",
    "            if (\"n_neurons_per_hidden\" in params.keys()):\n",
    "                n_neurons_per_hidden=params[\"n_neurons_per_hidden\"]\n",
    "        self.n_per_hidden = n_neurons_per_hidden\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.weights = None \n",
    "        self.n_weights = None\n",
    "        self.init_random_params()\n",
    "        self.out = np.zeros(n_out)\n",
    "        #print(\"Creating a simple mlp with %d inputs, %d outputs, %d hidden layers and %d neurons per layer\"%(n_in, n_out,n_hidden_layers, n_neurons_per_hidden))\n",
    "    def init_random_params(self):\n",
    "        if(self.n_hidden_layers > 0):\n",
    "            self.weights = [np.random.random((self.dim_in,self.n_per_hidden))] # In -> first hidden\n",
    "            self.bias = [np.random.random(self.n_per_hidden)] # In -> first hidden\n",
    "            for i in range(self.n_hidden_layers-1): # Hidden -> hidden\n",
    "                self.weights.append(np.random.random((self.n_per_hidden,self.n_per_hidden)))\n",
    "                self.bias.append(np.random.random(self.n_per_hidden))\n",
    "            self.weights.append(np.random.random((self.n_per_hidden,self.dim_out))) # -> last hidden -> out\n",
    "            self.bias.append(np.random.random(self.dim_out))\n",
    "        else:\n",
    "            self.weights = [np.random.random((self.dim_in,self.dim_out))] # Single-layer perceptron\n",
    "            self.bias = [np.random.random(self.dim_out)]\n",
    "        self.n_weights = np.sum([np.product(w.shape) for w in self.weights]) + np.sum([np.product(b.shape) for b in self.bias])\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns all network parameters as a single array\n",
    "        \"\"\"\n",
    "        flat_weights = np.hstack([arr.flatten() for arr in (self.weights+self.bias)])\n",
    "        return flat_weights\n",
    "\n",
    "    def set_parameters(self, flat_parameters):\n",
    "        \"\"\"\n",
    "        Set all network parameters from a single array\n",
    "        \"\"\"\n",
    "        i = 0 # index\n",
    "        to_set = []\n",
    "        self.weights = list()\n",
    "        self.bias = list()\n",
    "        if(self.n_hidden_layers > 0):\n",
    "            # In -> first hidden\n",
    "            w0 = np.array(flat_parameters[i:(i+self.dim_in*self.n_per_hidden)])\n",
    "            self.weights.append(w0.reshape(self.dim_in,self.n_per_hidden))\n",
    "            i += self.dim_in*self.n_per_hidden\n",
    "            for l in range(self.n_hidden_layers-1): # Hidden -> hidden\n",
    "                w = np.array(flat_parameters[i:(i+self.n_per_hidden*self.n_per_hidden)])\n",
    "                self.weights.append(w.reshape((self.n_per_hidden,self.n_per_hidden)))\n",
    "                i += self.n_per_hidden*self.n_per_hidden\n",
    "            # -> last hidden -> out\n",
    "            wN = np.array(flat_parameters[i:(i+self.n_per_hidden*self.dim_out)])\n",
    "            self.weights.append(wN.reshape((self.n_per_hidden,self.dim_out)))\n",
    "            i += self.n_per_hidden*self.dim_out\n",
    "            # Samefor bias now\n",
    "            # In -> first hidden\n",
    "            b0 = np.array(flat_parameters[i:(i+self.n_per_hidden)])\n",
    "            self.bias.append(b0)\n",
    "            i += self.n_per_hidden\n",
    "            for l in range(self.n_hidden_layers-1): # Hidden -> hidden\n",
    "                b = np.array(flat_parameters[i:(i+self.n_per_hidden)])\n",
    "                self.bias.append(b)\n",
    "                i += self.n_per_hidden\n",
    "            # -> last hidden -> out\n",
    "            bN = np.array(flat_parameters[i:(i+self.dim_out)])\n",
    "            self.bias.append(bN)\n",
    "            i += self.dim_out\n",
    "        else:\n",
    "            n_w = self.dim_in*self.dim_out\n",
    "            w = np.array(flat_parameters[:n_w])\n",
    "            self.weights = [w.reshape((self.dim_in,self.dim_out))]\n",
    "            self.bias = [np.array(flat_parameters[n_w:])]\n",
    "        self.n_weights = np.sum([np.product(w.shape) for w in self.weights]) + np.sum([np.product(b.shape) for b in self.bias])\n",
    "    \n",
    "    def predict(self,x):\n",
    "        if(self.n_hidden_layers > 0):\n",
    "            #Input\n",
    "            a = np.matmul(x,self.weights[0]) + self.bias[0]\n",
    "            #y = sigmoid(a)\n",
    "            y = a\n",
    "            # hidden -> hidden\n",
    "            for i in range(1,self.n_hidden_layers-1):\n",
    "                a = np.matmul(y, self.weights[i]) + self.bias[i]\n",
    "                y = sigmoid(a)\n",
    "            # Out\n",
    "            a = np.matmul(y, self.weights[-1]) + self.bias[-1]\n",
    "            \"\"\"\n",
    "            ICI !! \n",
    "            \"\"\"\n",
    "            #out = tanh(a)\n",
    "            out = a\n",
    "            return out\n",
    "        else: # Simple monolayer perceptron\n",
    "            \"\"\"\n",
    "            et ICI !! \n",
    "            \"\"\"\n",
    "            #return tanh(np.matmul(x,self.weights[0]) + self.bias[0])\n",
    "            return np.matmul(x,self.weights[0]) + self.bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_boston\n",
    "#On charge les données \n",
    "#On utilise un réseau de neurones simple \n",
    "Network = SimpleNeuralControllerNumpy(13, 1, n_hidden_layers=0, n_neurons_per_hidden=0, params=None)\n",
    "Network.init_random_params()\n",
    "print(Network.get_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La fonction d'evalutation qu'on prend va associer a chaque vecteur de poids w l'erreur moyenne quadratique\n",
    "\n",
    "def evaluation_function(w):\n",
    "    X, y = load_boston(return_X_y=True)\n",
    "    Network = SimpleNeuralControllerNumpy(13, 1, n_hidden_layers=0, n_neurons_per_hidden=0, params=None)\n",
    "    Network.set_parameters(w)\n",
    "    y_pred = Network.predict(X)\n",
    "    return mean_squared_error(y,y_pred)\n",
    "\n",
    "Network = SimpleNeuralControllerNumpy(13, 1, n_hidden_layers=0, n_neurons_per_hidden=0, params=None)\n",
    "Network.init_random_params()\n",
    "init_point = Network.get_parameters()\n",
    "evaluation_function(init_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_oneplusone(init_point, 0.01, evaluation_function, nbeval=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Petit rafinement de l'algorithme en ajoutant la règle des 1/5.\n",
    "Remarque : Ce rafinement contribue a réduire la stochacité de l'algorithme et donc sont utilisation doit être\n",
    "bien mesurée car elle le rend trés dépendant du point de départ\n",
    "\"\"\"\n",
    "\n",
    "def launch_ESonefifthRule(center, sigma, ma_func, nbeval=10000):\n",
    "    parent = np.array(center)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    parentFit = ma_func(parent)\n",
    "    bestFit = parentFit\n",
    "    bestIt = 0\n",
    "    print(\"Parent : \", parent)\n",
    "    while i < nbeval:\n",
    "        child = parent + sigma * np.random.normal(0, np.ones(parent.shape), parent.shape)\n",
    "        childFit = ma_func(child)\n",
    "        if childFit <= parentFit:\n",
    "            parentFit = childFit\n",
    "            parent = np.copy(child)\n",
    "            sigma = sigma*2\n",
    "            if bestFit > parentFit:\n",
    "                bestFit = parentFit\n",
    "                bestIt = i\n",
    "                print(\"New best fit found : \", bestFit,\" a l'itteration : \",i)\n",
    "\n",
    "        else:\n",
    "            sigma = sigma*(2**(-(1/4)))\n",
    "        j += 1\n",
    "        solutions = np.array([parent])\n",
    "        i += 1\n",
    "    print(\"Best fit\", bestFit, \"at iteration\", bestIt)\n",
    "    return bestFit\n",
    "\n",
    "\n",
    "launch_ESonefifthRule(init_point, 0.1, evaluation_function, nbeval=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_LambdaES(center, _lambda, sigma,ma_func, nbeval=10000):\n",
    "    center = np.array(center)\n",
    "    parent = center + sigma * np.random.normal(0, center.shape, center.shape[0])\n",
    "    i = 0\n",
    "    j = 0\n",
    "    parentFit = ma_func(parent)\n",
    "    bestFit = parentFit\n",
    "    bestIt = 0\n",
    "    print(\"Parent : \", parent)\n",
    "    while i < nbeval:\n",
    "        for k in range(_lambda):\n",
    "            child = parent + sigma * np.random.normal(0, np.ones(parent.shape), parent.shape)\n",
    "            childFit = ma_func(child)\n",
    "            if childFit <= parentFit:\n",
    "                parentFit = childFit\n",
    "                parent = np.copy(child)\n",
    "                sigma = sigma * 2\n",
    "                if bestFit > parentFit:\n",
    "                    bestFit = parentFit\n",
    "                    bestIt = i\n",
    "                    print(\"New best fit found : \", bestFit,\" a l'itteration : \",i)\n",
    "            else:\n",
    "                sigma = sigma * (2 ** (-(1 / 4)))\n",
    "            j += 1\n",
    "            solutions = np.array([parent])\n",
    "        i += 1\n",
    "    print(\"Best fit\", bestFit, \"at iteration\", bestIt)\n",
    "    return bestFit\n",
    "\n",
    "launch_LambdaES(init_point,100, 0.01, evaluation_function, nbeval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cma.CMAEvolutionStrategy(init_point, 0.01).optimize(evaluation_function,maxfun=10000).result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = purecma.fmin(evaluation_function,init_point,0.01,maxfevals=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (RLLab2)",
   "language": "python",
   "name": "pycharm-fe7a6dad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
